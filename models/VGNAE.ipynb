{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from logs import log\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GCNConv, GAE, VGAE, APPNP\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx, negative_sampling, to_networkx\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGNAE model version logs:\n",
    "    * ver1: VGNAE first try. \n",
    "    * ver2: Fix some problem.\n",
    "    * ver3: Use the training node embedding for validation & testing & uploading.\n",
    "    * ver4: Use the testing node embedding for testing & uploading.\n",
    "            (testing edge_index include 'train_data' & 'val_data' pos_edge_index)\n",
    "      Difference in ver3 & ver4: Need change eval_link_predictor() in inference phase and uploading phase.\n",
    "    * ver5: Use T.NormalizeFeatures()(data). (Normalize data.x) and use 1000 epoches.\n",
    "          |_2: like ver4 using testing node embedding.\n",
    "    * ver6: set embedding dimension = 64 (128 --> 64), lr=0.005, epoch=300\n",
    "          |_2: like ver4 using testing node embedding.\n",
    "    * ver7: (APPNP(K=1, alpha=0)) --> APPNP(K=10, alpha=0.15)\n",
    "### unVGNAE model version logs:\n",
    "    * ver1: Change to training on undirected graph. (embedding dimension = 64, lr=0.005)\n",
    "          |_2: Use the testing node embedding for testing & uploading.\n",
    "    * ver2: tune hyperparameter(APPNP alpha=0.15)\n",
    "          |_2: Use the testing node embedding for testing & uploading.\n",
    "    * ver3: tune hyperparameter(APPNP K=10) (ver2 is better)\n",
    "    * ver4: run experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Datasets:\n",
    "    * id: edge id, \n",
    "    * from & to: 'from' node point to 'to' node, \n",
    "    * label: connect or not.\n",
    "    * content: containing each node's attribute.\n",
    "\n",
    "   Evaluate:\n",
    "    * AUC: area under ROC curve\n",
    "    * AP: average precision\n",
    "\"\"\"\n",
    "data_path = './dataset1/'\n",
    "model_version = 'unVGNAE_ver4'\n",
    "upload_dataset_info = '_submission'\n",
    "store_file = model_version + upload_dataset_info\n",
    "tra_val_store_file = model_version + '_2' + upload_dataset_info\n",
    "log_file = 'logs/' + store_file + '.log'\n",
    "logger = log(path=data_path, file=log_file)\n",
    "\n",
    "df_train = pd.read_csv(data_path+'raw/train.csv').sort_values('from')\n",
    "df_test = pd.read_csv(data_path+'raw/test.csv')\n",
    "df_content = pd.read_csv(data_path+'raw/content.csv', delimiter='\\t', header=None)\n",
    "df_upload = pd.read_csv(data_path+'raw/upload.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature shape: (2708, 1434)\n"
     ]
    }
   ],
   "source": [
    "print(f'Node feature shape: {df_content.shape}')\n",
    "tmp_node_feats = df_content.set_index(0)\n",
    "tmp_node_ids = tmp_node_feats.index.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3157</th>\n",
       "      <td>E1276</td>\n",
       "      <td>962</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>E6057</td>\n",
       "      <td>527</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>E795</td>\n",
       "      <td>1937</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>E9763</td>\n",
       "      <td>839</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>E1266</td>\n",
       "      <td>590</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    to  from  label\n",
       "3157  E1276   962     1      1\n",
       "1723  E6057   527     2      0\n",
       "57     E795  1937     2      0\n",
       "4492  E9763   839     3      0\n",
       "4927  E1266   590     3      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(Graph_dataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['train.csv', 'content.csv']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).sort_values('from')\n",
    "        node_feats = pd.read_csv(self.raw_paths[1], delimiter='\\t', header=None, index_col=0)\n",
    "        \n",
    "        # Get node features. [num_nodes, num_node_features]\n",
    "        x = torch.tensor(node_feats.sort_index().values, dtype=torch.float)\n",
    "        \n",
    "        # Get positive data.(label = 1: link)\n",
    "        pos_data = self.data[self.data['label'] == 1]\n",
    "        # neg_data = self.data[self.data['label'] == 0]\n",
    "\n",
    "        # Get edge index.\n",
    "        graph = nx.from_pandas_edgelist(pos_data, 'from', 'to', edge_attr=None)\n",
    "\n",
    "        pair1 = [i[0] for i in graph.edges()]\n",
    "        pair2 = [i[1] for i in graph.edges()]\n",
    "        pos_edge_index = torch.LongTensor([pair1+pair2,pair2+pair1])\n",
    "\n",
    "        # Create Data object.\n",
    "        proc_graph = Data(x=x,\n",
    "                          edge_index=pos_edge_index,\n",
    "                          y=None)\n",
    "        print(proc_graph)\n",
    "\n",
    "        data, slices = self.collate([proc_graph])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = Graph_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 8472])\n",
      "2708\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[   1,    4,    4,  ..., 1402, 2362, 1210],\n",
      "        [ 962, 2062, 1547,  ..., 2690, 2691, 2697]])\n",
      "None\n",
      "1433\n"
     ]
    }
   ],
   "source": [
    "for times, data in enumerate(demo, 1):\n",
    "    print(data)\n",
    "    print(data.x.size(0))\n",
    "    print(data.x)\n",
    "    print(data.edge_index)\n",
    "    print(data.y)\n",
    "    print(data.num_node_features)\n",
    "\n",
    "    # using this to check whether data.edge_index is fulfilled data.x values.\n",
    "    data.validate(raise_on_error=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, edge_index):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, out_channels)\n",
    "        self.linear2 = nn.Linear(in_channels, out_channels)\n",
    "        self.propagate = APPNP(K=1, alpha=0.15)\n",
    "\n",
    "    def forward(self, x, edge_index, not_prop=0):\n",
    "        x_ = self.linear1(x)\n",
    "        x_ = self.propagate(x_, edge_index)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = F.normalize(x,p=2,dim=1) * 1.8\n",
    "        x = self.propagate(x, edge_index)\n",
    "        return x, x_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link_predictor(model, train_data, val_data, optimizer, n_epochs=200):\n",
    "    logger.info('Training Start')\n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "        loss = model.recon_loss(z, train_data.edge_index)\n",
    "        loss = loss + (1/train_data.num_nodes) * model.kl_loss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_auc, val_ap = eval_link_predictor(model, train_data, val_data, None)\n",
    "        if epoch % 10 == 0:\n",
    "            # print('Epoch: {:03d}, TRAIN LOSS: {:.4f}, VAL AUC: {:.4f}, VAL AP: {:.4f}'.format(epoch, loss, val_auc, val_ap))\n",
    "            logger.info(f'Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val AUC: {val_auc:.3f}, Val AP: {val_ap:.3f}')\n",
    "\n",
    "    logger.info('Training End --------------------------------')\n",
    "    return model\n",
    "\n",
    "def eval_link_predictor(model, train_data, val_data, test_data=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if test_data == None: \n",
    "            z = model.encode(train_data.x, train_data.edge_index)\n",
    "        else:\n",
    "            # 'test_data.edge_index' include 'train_data' & 'val_data' pos_edge_index.\n",
    "            z = model.encode(test_data.x, test_data.edge_index)\n",
    "\n",
    "    return model.test(z, val_data.pos_edge_label_index, val_data.neg_edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\mlg_ml\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "INFO Training Start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 8472])\n",
      "Data(x=[2708, 1433], edge_index=[2, 7204], pos_edge_label=[3602], pos_edge_label_index=[2, 3602])\n",
      "Data(x=[2708, 1433], edge_index=[2, 7204], pos_edge_label=[211], pos_edge_label_index=[2, 211], neg_edge_label=[211], neg_edge_label_index=[2, 211])\n",
      "Data(x=[2708, 1433], edge_index=[2, 7626], pos_edge_label=[423], pos_edge_label_index=[2, 423], neg_edge_label=[423], neg_edge_label_index=[2, 423])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9580de553ef847bf91ee578992b1ee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Epoch: 010, Train Loss: 12.633, Val AUC: 0.704, Val AP: 0.741\n",
      "INFO Epoch: 020, Train Loss: 11.176, Val AUC: 0.711, Val AP: 0.747\n",
      "INFO Epoch: 030, Train Loss: 9.726, Val AUC: 0.728, Val AP: 0.758\n",
      "INFO Epoch: 040, Train Loss: 8.050, Val AUC: 0.807, Val AP: 0.818\n",
      "INFO Epoch: 050, Train Loss: 6.472, Val AUC: 0.946, Val AP: 0.938\n",
      "INFO Epoch: 060, Train Loss: 5.132, Val AUC: 0.951, Val AP: 0.946\n",
      "INFO Epoch: 070, Train Loss: 4.545, Val AUC: 0.941, Val AP: 0.937\n",
      "INFO Epoch: 080, Train Loss: 3.947, Val AUC: 0.941, Val AP: 0.936\n",
      "INFO Epoch: 090, Train Loss: 3.357, Val AUC: 0.938, Val AP: 0.933\n",
      "INFO Epoch: 100, Train Loss: 3.064, Val AUC: 0.939, Val AP: 0.934\n",
      "INFO Epoch: 110, Train Loss: 2.850, Val AUC: 0.938, Val AP: 0.932\n",
      "INFO Epoch: 120, Train Loss: 2.528, Val AUC: 0.938, Val AP: 0.933\n",
      "INFO Epoch: 130, Train Loss: 2.440, Val AUC: 0.939, Val AP: 0.934\n",
      "INFO Epoch: 140, Train Loss: 2.265, Val AUC: 0.939, Val AP: 0.934\n",
      "INFO Epoch: 150, Train Loss: 2.157, Val AUC: 0.939, Val AP: 0.935\n",
      "INFO Epoch: 160, Train Loss: 2.036, Val AUC: 0.939, Val AP: 0.935\n",
      "INFO Epoch: 170, Train Loss: 1.900, Val AUC: 0.938, Val AP: 0.935\n",
      "INFO Epoch: 180, Train Loss: 1.847, Val AUC: 0.939, Val AP: 0.936\n",
      "INFO Epoch: 190, Train Loss: 1.792, Val AUC: 0.939, Val AP: 0.936\n",
      "INFO Epoch: 200, Train Loss: 1.732, Val AUC: 0.938, Val AP: 0.936\n",
      "INFO Epoch: 210, Train Loss: 1.642, Val AUC: 0.938, Val AP: 0.937\n",
      "INFO Epoch: 220, Train Loss: 1.602, Val AUC: 0.939, Val AP: 0.938\n",
      "INFO Epoch: 230, Train Loss: 1.575, Val AUC: 0.938, Val AP: 0.938\n",
      "INFO Epoch: 240, Train Loss: 1.528, Val AUC: 0.939, Val AP: 0.938\n",
      "INFO Epoch: 250, Train Loss: 1.535, Val AUC: 0.940, Val AP: 0.938\n",
      "INFO Epoch: 260, Train Loss: 1.465, Val AUC: 0.940, Val AP: 0.940\n",
      "INFO Epoch: 270, Train Loss: 1.434, Val AUC: 0.940, Val AP: 0.940\n",
      "INFO Epoch: 280, Train Loss: 1.384, Val AUC: 0.940, Val AP: 0.940\n",
      "INFO Epoch: 290, Train Loss: 1.395, Val AUC: 0.940, Val AP: 0.940\n",
      "INFO Epoch: 300, Train Loss: 1.335, Val AUC: 0.940, Val AP: 0.940\n",
      "INFO Epoch: 310, Train Loss: 1.337, Val AUC: 0.939, Val AP: 0.940\n",
      "INFO Epoch: 320, Train Loss: 1.334, Val AUC: 0.939, Val AP: 0.940\n",
      "INFO Epoch: 330, Train Loss: 1.297, Val AUC: 0.938, Val AP: 0.940\n",
      "INFO Epoch: 340, Train Loss: 1.266, Val AUC: 0.939, Val AP: 0.940\n",
      "INFO Epoch: 350, Train Loss: 1.284, Val AUC: 0.939, Val AP: 0.940\n",
      "INFO Epoch: 360, Train Loss: 1.229, Val AUC: 0.938, Val AP: 0.940\n",
      "INFO Epoch: 370, Train Loss: 1.239, Val AUC: 0.939, Val AP: 0.940\n",
      "INFO Epoch: 380, Train Loss: 1.233, Val AUC: 0.938, Val AP: 0.940\n",
      "INFO Epoch: 390, Train Loss: 1.208, Val AUC: 0.937, Val AP: 0.939\n",
      "INFO Epoch: 400, Train Loss: 1.216, Val AUC: 0.938, Val AP: 0.939\n",
      "INFO Training End --------------------------------\n",
      "INFO Test AUC: 0.947, Test AP: 0.952\n",
      "INFO Test AUC: 0.948, Test AP: 0.953\n"
     ]
    }
   ],
   "source": [
    "data = demo.data\n",
    "data = T.NormalizeFeatures()(data)\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(num_val=0.05, \n",
    "                                                    num_test=0.1, \n",
    "                                                    split_labels=True, \n",
    "                                                    is_undirected=True, \n",
    "                                                    add_negative_train_samples=False\n",
    "                                                    )(data)\n",
    "print(data)\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VGAE(Encoder(data.num_features, 64, train_data.edge_index)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "model = train_link_predictor(model, train_data, val_data, optimizer, n_epochs=300)\n",
    "\n",
    "test_auc, test_ap = eval_link_predictor(model, train_data, test_data, None)\n",
    "logger.info(f\"Test AUC: {test_auc:.3f}, Test AP: {test_ap:.3f}\")\n",
    "test_auc, test_ap = eval_link_predictor(model, train_data, test_data, test_data)\n",
    "logger.info(f\"Test AUC: {test_auc:.3f}, Test AP: {test_ap:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(data_path+'raw/test.csv')\n",
    "test_feats = pd.read_csv(data_path+'raw/content.csv', delimiter='\\t', header=None, index_col=0)\n",
    "test_x = torch.tensor(test_feats.sort_index().values, dtype=torch.float)\n",
    "test_id = test_df['id'].values\n",
    "test_edge_index = torch.tensor(test_df[['from', 'to']].values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4536, 0.6662, 0.8894,  ..., 0.9388, 0.9649, 0.3684])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    out = model.decode(z, test_edge_index).view(-1)\n",
    "    print(out)\n",
    "    # out = torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['id', 'prob']\n",
    "out = out.numpy()\n",
    "output_csv = []\n",
    "for ind, val in enumerate(test_id):\n",
    "    output_csv.append([val, str(out[ind])])\n",
    "output_csv = pd.DataFrame(output_csv, columns=header)\n",
    "output_csv.to_csv(data_path+'submission/'+store_file+'.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tra_val inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(data_path+'raw/test.csv')\n",
    "test_feats = pd.read_csv(data_path+'raw/content.csv', delimiter='\\t', header=None, index_col=0)\n",
    "test_x = torch.tensor(test_feats.sort_index().values, dtype=torch.float)\n",
    "test_id = test_df['id'].values\n",
    "test_edge_index = torch.tensor(test_df[['from', 'to']].values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4695, 0.6662, 0.8894,  ..., 0.9388, 0.9611, 0.3684])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(test_data.x, test_data.edge_index)\n",
    "    out = model.decode(z, test_edge_index).view(-1)\n",
    "    print(out)\n",
    "    # out = torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['id', 'prob']\n",
    "out = out.numpy()\n",
    "output_csv = []\n",
    "for ind, val in enumerate(test_id):\n",
    "    output_csv.append([val, str(out[ind])])\n",
    "output_csv = pd.DataFrame(output_csv, columns=header)\n",
    "output_csv.to_csv(data_path+'submission/'+tra_val_store_file+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64049391f96ba131a9e04c522b3e94cd43efdce572641ac76d85c52ad35b8cda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
